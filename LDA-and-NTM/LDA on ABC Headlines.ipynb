{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and processing data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip -q install gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = 100000\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv.gz', compression='gzip',\n",
    "                      error_bad_lines=False, dtype='str', nrows=num_lines)\n",
    "\n",
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85287</th>\n",
       "      <td>20040416</td>\n",
       "      <td>thousands flee as suva floods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81984</th>\n",
       "      <td>20040331</td>\n",
       "      <td>mitsubishi mulls australian pullout report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11164</th>\n",
       "      <td>20030413</td>\n",
       "      <td>vic govt confronts youth alcohol abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77036</th>\n",
       "      <td>20040308</td>\n",
       "      <td>boy dies in wudinna farm tragedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>20030426</td>\n",
       "      <td>polls open for new maryborough mp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      publish_date                               headline_text\n",
       "85287     20040416               thousands flee as suva floods\n",
       "81984     20040331  mitsubishi mulls australian pullout report\n",
       "11164     20030413      vic govt confronts youth alcohol abuse\n",
       "77036     20040308            boy dies in wudinna farm tragedy\n",
       "13852     20030426           polls open for new maryborough mp"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['publish_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, '')\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    text = text.lower().split()\n",
    "    text = [w for w in text if not w in stop_words] \n",
    "    text = [wnl.lemmatize(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.15 s, sys: 22.4 ms, total: 4.17 s\n",
      "Wall time: 4.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['headline_text'] = data['headline_text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85287</th>\n",
       "      <td>[thousand, flee, suva, flood]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81984</th>\n",
       "      <td>[mitsubishi, mull, australian, pullout, report]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11164</th>\n",
       "      <td>[vic, govt, confronts, youth, alcohol, abuse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77036</th>\n",
       "      <td>[boy, dy, wudinna, farm, tragedy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>[poll, open, new, maryborough, mp]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         headline_text\n",
       "85287                    [thousand, flee, suva, flood]\n",
       "81984  [mitsubishi, mull, australian, pullout, report]\n",
       "11164    [vic, govt, confronts, youth, alcohol, abuse]\n",
       "77036                [boy, dy, wudinna, farm, tragedy]\n",
       "13852               [poll, open, new, maryborough, mp]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 17.2 ms, total: 1.17 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(data['headline_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(22976 unique tokens: ['flee', 'flood', 'suva', 'thousand', 'australian']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(512 unique tokens: ['flood', 'thousand', 'australian', 'report', 'abuse']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(keep_n=512)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for index in range(0,len(dictionary)):\n",
    "        f.write(dictionary.get(index)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read dictionary from vocab.txt\n",
    "\n",
    "#--------------------OF no use------------------------\n",
    "\n",
    "vocab_file = open(\"vocab.txt\", \"r\")\n",
    "dictionary =vocab_file.read()\n",
    "dictionary = dictionary.split(\"\\n\")\n",
    "vocab_file.close()\n",
    "#dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data['tokens'] = data.apply(lambda row: dictionary.doc2bow(row['headline_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['headline_text'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0rc1\n"
     ]
    }
   ],
   "source": [
    "import io, boto3\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'headlines-lda-ntm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_protobuf_dataset(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    token_matrix = lil_matrix((num_lines, num_columns)).astype('float32')\n",
    "    line = 0\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            token_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "        \n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, token_matrix, None)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_protbuf_dataset(buf, bucket, prefix, key):\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    buf.seek(0)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(training_buf)\n",
    "    path = 's3://{}/{}'.format(bucket,obj)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_training_path='s3://sagemaker-us-east-1-886035371869/sagemaker-scikit-learn-2021-03-27-17-18-09-298/output/train_data/training.protobuf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "training_buf = build_protobuf_dataset(data, dictionary)\n",
    "s3_training_path = upload_protbuf_dataset(training_buf, bucket, prefix, 'training/training.protobuf')\n",
    "print(s3_training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_auxiliary_path = session.upload_data(path='vocab.txt', key_prefix=prefix + '/input/auxiliary')\n",
    "print(s3_auxiliary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-886035371869/headlines-lda-ntm/output/\n"
     ]
    }
   ],
   "source": [
    "s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "\n",
    "print(s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766337827248.dkr.ecr.us-east-1.amazonaws.com/lda:1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "container = image_uris.retrieve('lda', region)\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "lda = sagemaker.estimator.Estimator(container,\n",
    "                                   role,\n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.c5.2xlarge',\n",
    "                                   output_path=s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.set_hyperparameters(num_topics=10, \n",
    "                        #feature_dim=len(dictionary), \n",
    "                        feature_dim=512,\n",
    "                        #mini_batch_size=num_lines,\n",
    "                        mini_batch_size=97320,\n",
    "                        alpha0=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(inputs={'train': s3_training_path},wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TrainingJobName': 'lda-2021-03-27-17-52-08-679', 'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:886035371869:training-job/lda-2021-03-27-17-52-08-679', 'ModelArtifacts': {'S3ModelArtifacts': 's3://sagemaker-us-east-1-886035371869/headlines-lda-ntm/output/lda-2021-03-27-17-52-08-679/output/model.tar.gz'}, 'TrainingJobStatus': 'Completed', 'SecondaryStatus': 'Completed', 'HyperParameters': {'alpha0': '0.1', 'feature_dim': '512', 'mini_batch_size': '97320', 'num_topics': '10'}, 'AlgorithmSpecification': {'TrainingImage': '766337827248.dkr.ecr.us-east-1.amazonaws.com/lda:1', 'TrainingInputMode': 'File', 'MetricDefinitions': [{'Name': 'train:progress', 'Regex': '#progress_metric: host=\\\\S+, completed (\\\\S+) %'}, {'Name': 'test:pwll', 'Regex': '#quality_metric: host=\\\\S+, test pwll <score>=(\\\\S+)'}, {'Name': 'train:throughput', 'Regex': '#throughput_metric: host=\\\\S+, train throughput=(\\\\S+) records/second'}], 'EnableSageMakerMetricsTimeSeries': False}, 'RoleArn': 'arn:aws:iam::886035371869:role/service-role/AmazonSageMaker-ExecutionRole-20200914T135573', 'InputDataConfig': [{'ChannelName': 'train', 'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-886035371869/sagemaker-scikit-learn-2021-03-27-17-18-09-298/output/train_data/training.protobuf', 'S3DataDistributionType': 'FullyReplicated'}}, 'CompressionType': 'None', 'RecordWrapperType': 'None'}], 'OutputDataConfig': {'KmsKeyId': '', 'S3OutputPath': 's3://sagemaker-us-east-1-886035371869/headlines-lda-ntm/output/'}, 'ResourceConfig': {'InstanceType': 'ml.c5.2xlarge', 'InstanceCount': 1, 'VolumeSizeInGB': 30}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'CreationTime': datetime.datetime(2021, 3, 27, 17, 52, 8, 899000, tzinfo=tzlocal()), 'TrainingStartTime': datetime.datetime(2021, 3, 27, 17, 54, 1, 159000, tzinfo=tzlocal()), 'TrainingEndTime': datetime.datetime(2021, 3, 27, 17, 55, 9, 240000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2021, 3, 27, 17, 55, 9, 240000, tzinfo=tzlocal()), 'SecondaryStatusTransitions': [{'Status': 'Starting', 'StartTime': datetime.datetime(2021, 3, 27, 17, 52, 8, 899000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 3, 27, 17, 54, 1, 159000, tzinfo=tzlocal()), 'StatusMessage': 'Preparing the instances for training'}, {'Status': 'Downloading', 'StartTime': datetime.datetime(2021, 3, 27, 17, 54, 1, 159000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 3, 27, 17, 54, 20, 497000, tzinfo=tzlocal()), 'StatusMessage': 'Downloading input data'}, {'Status': 'Training', 'StartTime': datetime.datetime(2021, 3, 27, 17, 54, 20, 497000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 3, 27, 17, 55, 2, 389000, tzinfo=tzlocal()), 'StatusMessage': 'Training image download completed. Training in progress.'}, {'Status': 'Uploading', 'StartTime': datetime.datetime(2021, 3, 27, 17, 55, 2, 389000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 3, 27, 17, 55, 9, 240000, tzinfo=tzlocal()), 'StatusMessage': 'Uploading generated training model'}, {'Status': 'Completed', 'StartTime': datetime.datetime(2021, 3, 27, 17, 55, 9, 240000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2021, 3, 27, 17, 55, 9, 240000, tzinfo=tzlocal()), 'StatusMessage': 'Training job completed'}], 'FinalMetricDataList': [{'MetricName': 'train:progress', 'Value': 100.0, 'Timestamp': datetime.datetime(2021, 3, 27, 17, 54, 57, tzinfo=tzlocal())}, {'MetricName': 'train:throughput', 'Value': 50399.0625, 'Timestamp': datetime.datetime(2021, 3, 27, 17, 54, 57, tzinfo=tzlocal())}], 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False, 'EnableManagedSpotTraining': False, 'TrainingTimeInSeconds': 68, 'BillableTimeInSeconds': 68, 'ResponseMetadata': {'RequestId': '816860ce-7af1-444f-80f0-bbd93be86b27', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '816860ce-7af1-444f-80f0-bbd93be86b27', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3206', 'date': 'Sat, 27 Mar 2021 17:55:38 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "sm_client=boto3.client('sagemaker')\n",
    "response = sm_client.describe_training_job(\n",
    "    TrainingJobName='lda-2021-03-27-17-52-08-679'\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "lda_predictor = lda.deploy(initial_instance_count=1, instance_type='ml.t2.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_samples(samples, dictionary):\n",
    "    num_lines = len(samples)\n",
    "    num_columns = len(dictionary)\n",
    "    sample_matrix = np.zeros((num_lines, num_columns)).astype('float32')\n",
    "    for line in range(0, num_lines):\n",
    "        s = samples[line]\n",
    "        s = process_text(s)\n",
    "        s = dictionary.doc2bow(s)\n",
    "        for token_id, token_count in s:\n",
    "            sample_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "    return sample_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to try your own samples\n",
    "\n",
    "samples = [\n",
    "    \"Major tariffs expected to end Australian barley trade to China\",\n",
    "    \"US woman wanted over fatal crash asks for release after coronavirus halts extradition\",\n",
    "    \"Fifty trains out of service as fault forces Adelaide passengers to 'pack like sardines\",\n",
    "    \"Germany's Bundesliga plans its return from lockdown as football world watches\",\n",
    "    \"All AFL players to face COVID-19 testing before training resumes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in praise of the make under'\n",
      " 'tas considers incentives to cut live exports'\n",
      " 'locals compare damage caused by cyclone larry to'\n",
      " 'grieving father confronts health minister at grafton'\n",
      " 'wild dog detection']\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to load 5 random samples from the dataset\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv.gz', compression='gzip',\n",
    "                      error_bad_lines=False, dtype='str')\n",
    "samples = data.sample(frac=1)[:5]\n",
    "samples = np.array(samples.headline_text)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"predictions\": [{\"topic_mixture\": [0.5590918064117432, 0.0, 0.4409082531929016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, {\"topic_mixture\": [0.6972892880439758, 0.3027106821537018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, {\"topic_mixture\": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, {\"topic_mixture\": [0.6785109043121338, 0.0, 0.0, 0.0, 0.3214890658855438, 0.0, 0.0, 0.0, 0.0, 0.0]}, {\"topic_mixture\": [0.767515242099762, 0.23248475790023804, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}]}'\n"
     ]
    }
   ],
   "source": [
    "lda_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "response = lda_predictor.predict(process_samples(samples, dictionary))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "response = json.loads(response)\n",
    "vectors = [r['topic_mixture'] for r in response['predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0, 0.56\n",
      "topic 0, 0.70\n",
      "topic 0, 1.00\n",
      "topic 0, 0.68\n",
      "topic 0, 0.77\n"
     ]
    }
   ],
   "source": [
    "for v in vectors:\n",
    "    top_topic = np.argmax(v)\n",
    "    print(\"topic %s, %2.2f\" % (top_topic, v[top_topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (SageMaker JumpStart PyTorch 1.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:793310587911:image/sagemaker-jumpstart-pytorch-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
