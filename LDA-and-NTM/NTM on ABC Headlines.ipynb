{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and processing data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip -q install --upgrade pip\n",
    "pip -q install gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = 1000000\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv.gz', compression='gzip',\n",
    "                      error_bad_lines=False, dtype='str', nrows=num_lines)\n",
    "\n",
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>351196</th>\n",
       "      <td>20071217</td>\n",
       "      <td>us rejects turkish air strike approval claims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958513</th>\n",
       "      <td>20150616</td>\n",
       "      <td>berg tpp not the bogey treaty that we think it is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394335</th>\n",
       "      <td>20080702</td>\n",
       "      <td>irrigators get water allocations cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603197</th>\n",
       "      <td>20110325</td>\n",
       "      <td>four guilty over thurston uncles bashing death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128865</th>\n",
       "      <td>20041121</td>\n",
       "      <td>fear prevents sudanese returning to darfur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       publish_date                                      headline_text\n",
       "351196     20071217      us rejects turkish air strike approval claims\n",
       "958513     20150616  berg tpp not the bogey treaty that we think it is\n",
       "394335     20080702               irrigators get water allocations cut\n",
       "603197     20110325     four guilty over thurston uncles bashing death\n",
       "128865     20041121         fear prevents sudanese returning to darfur"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['publish_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, '')\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    text = text.lower().split()\n",
    "    text = [w for w in text if not w in stop_words] \n",
    "    text = [wnl.lemmatize(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.2 s, sys: 442 ms, total: 41.7 s\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['headline_text'] = data['headline_text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>351196</th>\n",
       "      <td>[u, reject, turkish, air, strike, approval, cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958513</th>\n",
       "      <td>[berg, tpp, bogey, treaty, think]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394335</th>\n",
       "      <td>[irrigators, get, water, allocation, cut]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603197</th>\n",
       "      <td>[four, guilty, thurston, uncle, bashing, death]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128865</th>\n",
       "      <td>[fear, prevents, sudanese, returning, darfur]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline_text\n",
       "351196  [u, reject, turkish, air, strike, approval, cl...\n",
       "958513                  [berg, tpp, bogey, treaty, think]\n",
       "394335          [irrigators, get, water, allocation, cut]\n",
       "603197    [four, guilty, thurston, uncle, bashing, death]\n",
       "128865      [fear, prevents, sudanese, returning, darfur]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.local/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 35.3 ms, total: 11.6 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(data['headline_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(74759 unique tokens: ['air', 'approval', 'claim', 'reject', 'strike']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(512 unique tokens: ['air', 'claim', 'reject', 'strike', 'u']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(keep_n=512, no_above=0.5)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for index in range(0,len(dictionary)):\n",
    "        f.write(dictionary.get(index)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 204 ms, total: 12.4 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data['tokens'] = data.apply(lambda row: dictionary.doc2bow(row['headline_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>351196</th>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958513</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394335</th>\n",
       "      <td>[(5, 1), (6, 1), (7, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603197</th>\n",
       "      <td>[(8, 1), (9, 1), (10, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128865</th>\n",
       "      <td>[(11, 1)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tokens\n",
       "351196  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n",
       "958513                                        []\n",
       "394335                  [(5, 1), (6, 1), (7, 1)]\n",
       "603197                 [(8, 1), (9, 1), (10, 1)]\n",
       "128865                                 [(11, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['headline_text'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0rc1\n"
     ]
    }
   ],
   "source": [
    "import io, boto3\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'headlines-lda-ntm-NTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_protobuf_dataset(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    token_matrix = lil_matrix((num_lines, num_columns)).astype('float32')\n",
    "    line = 0\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            token_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "        \n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, token_matrix, None)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_protbuf_dataset(buf, bucket, prefix, key):\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    buf.seek(0)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(training_buf)\n",
    "    path = 's3://{}/{}'.format(bucket,obj)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-886035371869/headlines-lda-ntm-NTM/training/training.protobuf\n",
      "CPU times: user 2min 31s, sys: 2.06 s, total: 2min 33s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_buf = build_protobuf_dataset(data, dictionary)\n",
    "s3_training_path = upload_protbuf_dataset(training_buf, bucket, prefix, 'training/training.protobuf')\n",
    "print(s3_training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-886035371869/headlines-lda-ntm-NTM/input/auxiliary/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "s3_auxiliary_path = session.upload_data(path='vocab.txt', key_prefix=prefix + '/input/auxiliary')\n",
    "print(s3_auxiliary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-886035371869/headlines-lda-ntm-NTM/output/\n"
     ]
    }
   ],
   "source": [
    "s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "\n",
    "print(s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382416733822.dkr.ecr.us-east-1.amazonaws.com/ntm:1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "container = image_uris.retrieve('ntm', region)\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                   role, \n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.p3.2xlarge',\n",
    "                                   sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm.set_hyperparameters(num_topics=10, \n",
    "                        feature_dim=len(dictionary),\n",
    "                        optimizer='adam', \n",
    "                        mini_batch_size=256,\n",
    "                        epochs=100,\n",
    "                        num_patience_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-27 18:17:37 Starting - Starting the training job...\n",
      "2021-03-27 18:17:39 Starting - Launching requested ML instances......\n",
      "2021-03-27 18:19:08 Starting - Preparing the instances for training.........\n",
      "2021-03-27 18:20:25 Downloading - Downloading input data\n",
      "2021-03-27 18:20:25 Training - Downloading the training image...\n",
      "2021-03-27 18:21:07 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:09 INFO 140212526044992] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:09 INFO 140212526044992] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '512', 'optimizer': 'adam', 'num_topics': '10', 'num_patience_epochs': '10', 'epochs': '100', 'mini_batch_size': '256'}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:09 INFO 140212526044992] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '100', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adam', 'tolerance': '0.001', 'num_patience_epochs': '10', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '512', 'num_topics': '10'}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:10 INFO 140212526044992] nvidia-smi took: 0.10050415992736816 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:10 INFO 140212526044992] Using default worker.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:10 INFO 140212526044992] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:10 INFO 140212526044992] Initializing\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:10 INFO 140212526044992] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:10 INFO 140212526044992] vocab.txt\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:10 INFO 140212526044992] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:10 INFO 140212526044992] Loading pre-trained token embedding vectors from /opt/amazon/lib/python3.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:19 WARNING 140212526044992] 0 out of 512 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:19 INFO 140212526044992] Vocab embedding shape: (512, 50)\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:19 INFO 140212526044992] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:28 INFO 140212526044992] Create Store: device\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869288.242606, \"EndTime\": 1616869288.2426414, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:21:28.245] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 18140, \"num_examples\": 1, \"num_bytes\": 11244}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:28 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:28 INFO 140212526044992] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:21:39.522] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 11276, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] # Finished training epoch 1 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] Loss (name: value) total: 5.479982628949255\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] Loss (name: value) kld: 0.12560451238812975\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] Loss (name: value) recons: 5.3543781219982325\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] Loss (name: value) logppx: 5.479982628949255\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=5.479982628949255\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] Timing: train: 11.28s, val: 0.00s, epoch: 11.28s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869288.2458007, \"EndTime\": 1616869299.5286503, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Total Batches Seen\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=88628.78158774793 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:39 INFO 140212526044992] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:21:51.085] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 11556, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] # Finished training epoch 2 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] Loss (name: value) total: 5.439725121421707\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] Loss (name: value) kld: 0.29809766694701234\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] Loss (name: value) recons: 5.141627458467916\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] Loss (name: value) logppx: 5.439725121421707\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=5.439725121421707\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] Timing: train: 11.56s, val: 0.00s, epoch: 11.56s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869299.5289555, \"EndTime\": 1616869311.090304, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2000000.0, \"count\": 1, \"min\": 2000000, \"max\": 2000000}, \"Total Batches Seen\": {\"sum\": 7814.0, \"count\": 1, \"min\": 7814, \"max\": 7814}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=86494.05211926208 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:21:51 INFO 140212526044992] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:22:02.431] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 11340, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] # Finished training epoch 3 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] Loss (name: value) total: 5.428665541339353\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] Loss (name: value) kld: 0.3627565363796281\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] Loss (name: value) recons: 5.065909008708854\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] Loss (name: value) logppx: 5.428665541339353\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=5.428665541339353\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] Timing: train: 11.34s, val: 0.00s, epoch: 11.35s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869311.0905814, \"EndTime\": 1616869322.4375036, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3000000.0, \"count\": 1, \"min\": 3000000, \"max\": 3000000}, \"Total Batches Seen\": {\"sum\": 11721.0, \"count\": 1, \"min\": 11721, \"max\": 11721}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=88128.36072706638 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:02 INFO 140212526044992] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:22:13.937] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 11499, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] # Finished training epoch 4 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] Loss (name: value) total: 5.424998447051096\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] Loss (name: value) kld: 0.3915660528421768\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] Loss (name: value) recons: 5.0334323946093855\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] Loss (name: value) logppx: 5.424998447051096\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=5.424998447051096\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] Timing: train: 11.50s, val: 0.00s, epoch: 11.50s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869322.4377928, \"EndTime\": 1616869333.9427516, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4000000.0, \"count\": 1, \"min\": 4000000, \"max\": 4000000}, \"Total Batches Seen\": {\"sum\": 15628.0, \"count\": 1, \"min\": 15628, \"max\": 15628}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=86917.74126322941 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:13 INFO 140212526044992] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:22:25.108] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 11164, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] # Finished training epoch 5 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] Loss (name: value) total: 5.423747243495558\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] Loss (name: value) kld: 0.4070720007528208\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] Loss (name: value) recons: 5.0166752389364\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] Loss (name: value) logppx: 5.423747243495558\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=5.423747243495558\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] Timing: train: 11.17s, val: 0.00s, epoch: 11.17s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869333.9430952, \"EndTime\": 1616869345.1130266, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5000000.0, \"count\": 1, \"min\": 5000000, \"max\": 5000000}, \"Total Batches Seen\": {\"sum\": 19535.0, \"count\": 1, \"min\": 19535, \"max\": 19535}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89524.6447176145 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:25 INFO 140212526044992] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:22:36.258] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 11145, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] # Finished training epoch 6 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] Loss (name: value) total: 5.421953913193393\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] Loss (name: value) kld: 0.4195808116351365\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] Loss (name: value) recons: 5.002373099632204\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] Loss (name: value) logppx: 5.421953913193393\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=5.421953913193393\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] Timing: train: 11.15s, val: 0.00s, epoch: 11.15s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869345.113332, \"EndTime\": 1616869356.2637482, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6000000.0, \"count\": 1, \"min\": 6000000, \"max\": 6000000}, \"Total Batches Seen\": {\"sum\": 23442.0, \"count\": 1, \"min\": 23442, \"max\": 23442}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89681.54539738233 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:36 INFO 140212526044992] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:22:47.479] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 11214, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] # Finished training epoch 7 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] Loss (name: value) total: 5.419592938125362\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] Loss (name: value) kld: 0.43269180589612255\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] Loss (name: value) recons: 4.98690112945458\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] Loss (name: value) logppx: 5.419592938125362\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=5.419592938125362\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] Timing: train: 11.22s, val: 0.00s, epoch: 11.22s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869356.2640605, \"EndTime\": 1616869367.4842138, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7000000.0, \"count\": 1, \"min\": 7000000, \"max\": 7000000}, \"Total Batches Seen\": {\"sum\": 27349.0, \"count\": 1, \"min\": 27349, \"max\": 27349}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89124.1183322716 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:47 INFO 140212526044992] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:22:58.784] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 11299, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] # Finished training epoch 8 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] Loss (name: value) total: 5.418350346472247\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] Loss (name: value) kld: 0.4393013975150847\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] Loss (name: value) recons: 4.979048941395977\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] Loss (name: value) logppx: 5.418350346472247\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=5.418350346472247\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] Timing: train: 11.30s, val: 0.00s, epoch: 11.31s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869367.4845388, \"EndTime\": 1616869378.790024, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 8000000.0, \"count\": 1, \"min\": 8000000, \"max\": 8000000}, \"Total Batches Seen\": {\"sum\": 31256.0, \"count\": 1, \"min\": 31256, \"max\": 31256}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=88451.4745546673 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:22:58 INFO 140212526044992] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:23:10.291] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 11500, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] # Finished training epoch 9 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] Loss (name: value) total: 5.417304898699684\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] Loss (name: value) kld: 0.44457146650862445\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] Loss (name: value) recons: 4.972733425764528\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] Loss (name: value) logppx: 5.417304898699684\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=5.417304898699684\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] Timing: train: 11.50s, val: 0.00s, epoch: 11.51s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869378.7903216, \"EndTime\": 1616869390.2968802, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9000000.0, \"count\": 1, \"min\": 9000000, \"max\": 9000000}, \"Total Batches Seen\": {\"sum\": 35163.0, \"count\": 1, \"min\": 35163, \"max\": 35163}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=86905.59220573427 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:10 INFO 140212526044992] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:23:21.588] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 11291, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] # Finished training epoch 10 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] Loss (name: value) total: 5.416238460694551\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] Loss (name: value) kld: 0.4495227888726139\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] Loss (name: value) recons: 4.966715672939428\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] Loss (name: value) logppx: 5.416238460694551\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=5.416238460694551\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] Timing: train: 11.29s, val: 0.00s, epoch: 11.30s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869390.2972295, \"EndTime\": 1616869401.5936086, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 10000000.0, \"count\": 1, \"min\": 10000000, \"max\": 10000000}, \"Total Batches Seen\": {\"sum\": 39070.0, \"count\": 1, \"min\": 39070, \"max\": 39070}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=88522.82221623343 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:21 INFO 140212526044992] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:23:32.663] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 11069, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] # Finished training epoch 11 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] Loss (name: value) total: 5.415423709595023\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] Loss (name: value) kld: 0.4524623189900487\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] Loss (name: value) recons: 4.96296139144214\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] Loss (name: value) logppx: 5.415423709595023\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=5.415423709595023\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] patience losses:[5.479982628949255, 5.439725121421707, 5.428665541339353, 5.424998447051096, 5.423747243495558, 5.421953913193393, 5.419592938125362, 5.418350346472247, 5.417304898699684, 5.416238460694551] min patience loss:5.416238460694551 current loss:5.415423709595023 absolute loss difference:0.0008147510995275553\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] Timing: train: 11.07s, val: 0.00s, epoch: 11.07s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869401.5939264, \"EndTime\": 1616869412.6683867, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 11000000.0, \"count\": 1, \"min\": 11000000, \"max\": 11000000}, \"Total Batches Seen\": {\"sum\": 42977.0, \"count\": 1, \"min\": 42977, \"max\": 42977}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=90296.40801563009 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:32 INFO 140212526044992] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:23:43.928] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 11259, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] # Finished training epoch 12 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] Loss (name: value) total: 5.415955349158411\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] Loss (name: value) kld: 0.45476009791117666\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] Loss (name: value) recons: 4.961195249035135\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] Loss (name: value) logppx: 5.415955349158411\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=5.415955349158411\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] patience losses:[5.439725121421707, 5.428665541339353, 5.424998447051096, 5.423747243495558, 5.421953913193393, 5.419592938125362, 5.418350346472247, 5.417304898699684, 5.416238460694551, 5.415423709595023] min patience loss:5.415423709595023 current loss:5.415955349158411 absolute loss difference:0.0005316395633876425\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] Timing: train: 11.26s, val: 0.00s, epoch: 11.26s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869412.668705, \"EndTime\": 1616869423.9294972, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 12000000.0, \"count\": 1, \"min\": 12000000, \"max\": 12000000}, \"Total Batches Seen\": {\"sum\": 46884.0, \"count\": 1, \"min\": 46884, \"max\": 46884}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=88802.35039439338 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:43 INFO 140212526044992] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:23:55.121] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 11191, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] # Finished training epoch 13 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] Loss (name: value) total: 5.414490560817694\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] Loss (name: value) kld: 0.45811393528619876\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] Loss (name: value) recons: 4.956376636885669\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] Loss (name: value) logppx: 5.414490560817694\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=5.414490560817694\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] patience losses:[5.428665541339353, 5.424998447051096, 5.423747243495558, 5.421953913193393, 5.419592938125362, 5.418350346472247, 5.417304898699684, 5.416238460694551, 5.415423709595023, 5.415955349158411] min patience loss:5.415423709595023 current loss:5.414490560817694 absolute loss difference:0.0009331487773289737\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] Timing: train: 11.19s, val: 0.00s, epoch: 11.20s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869423.929806, \"EndTime\": 1616869435.126335, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 13000000.0, \"count\": 1, \"min\": 13000000, \"max\": 13000000}, \"Total Batches Seen\": {\"sum\": 50791.0, \"count\": 1, \"min\": 50791, \"max\": 50791}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89312.30823280766 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:23:55 INFO 140212526044992] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:24:06.594] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 11467, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] # Finished training epoch 14 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] Loss (name: value) total: 5.414576234706442\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] Loss (name: value) kld: 0.4590981984446657\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] Loss (name: value) recons: 4.955478036925406\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] Loss (name: value) logppx: 5.414576234706442\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=5.414576234706442\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] patience losses:[5.424998447051096, 5.423747243495558, 5.421953913193393, 5.419592938125362, 5.418350346472247, 5.417304898699684, 5.416238460694551, 5.415423709595023, 5.415955349158411, 5.414490560817694] min patience loss:5.414490560817694 current loss:5.414576234706442 absolute loss difference:8.56738887478059e-05\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] Timing: train: 11.47s, val: 0.00s, epoch: 11.47s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869435.1266193, \"EndTime\": 1616869446.5959713, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 14000000.0, \"count\": 1, \"min\": 14000000, \"max\": 14000000}, \"Total Batches Seen\": {\"sum\": 54698.0, \"count\": 1, \"min\": 54698, \"max\": 54698}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=87187.77284479402 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:06 INFO 140212526044992] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:24:17.826] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 11230, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] # Finished training epoch 15 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] Loss (name: value) total: 5.41420695636472\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] Loss (name: value) kld: 0.4600176931453508\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] Loss (name: value) recons: 4.954189267004729\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] Loss (name: value) logppx: 5.41420695636472\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=5.41420695636472\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] patience losses:[5.423747243495558, 5.421953913193393, 5.419592938125362, 5.418350346472247, 5.417304898699684, 5.416238460694551, 5.415423709595023, 5.415955349158411, 5.414490560817694, 5.414576234706442] min patience loss:5.414490560817694 current loss:5.41420695636472 absolute loss difference:0.0002836044529743731\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] Timing: train: 11.23s, val: 0.00s, epoch: 11.24s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869446.596263, \"EndTime\": 1616869457.8317678, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 15000000.0, \"count\": 1, \"min\": 15000000, \"max\": 15000000}, \"Total Batches Seen\": {\"sum\": 58605.0, \"count\": 1, \"min\": 58605, \"max\": 58605}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89002.27238544112 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:17 INFO 140212526044992] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:24:29.068] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 11236, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] # Finished training epoch 16 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] Loss (name: value) total: 5.414015456298015\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] Loss (name: value) kld: 0.46388569347024544\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] Loss (name: value) recons: 4.950129758466502\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] Loss (name: value) logppx: 5.414015456298015\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=5.414015456298015\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] patience losses:[5.421953913193393, 5.419592938125362, 5.418350346472247, 5.417304898699684, 5.416238460694551, 5.415423709595023, 5.415955349158411, 5.414490560817694, 5.414576234706442, 5.41420695636472] min patience loss:5.41420695636472 current loss:5.414015456298015 absolute loss difference:0.0001915000667045419\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] Timing: train: 11.24s, val: 0.00s, epoch: 11.24s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869457.832063, \"EndTime\": 1616869469.0739696, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 16000000.0, \"count\": 1, \"min\": 16000000, \"max\": 16000000}, \"Total Batches Seen\": {\"sum\": 62512.0, \"count\": 1, \"min\": 62512, \"max\": 62512}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=88951.39777711002 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:29 INFO 140212526044992] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:24:40.189] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 11114, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] # Finished training epoch 17 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] Loss (name: value) total: 5.412800548019878\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] Loss (name: value) kld: 0.46363677661880254\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] Loss (name: value) recons: 4.949163760618997\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] Loss (name: value) logppx: 5.412800548019878\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=5.412800548019878\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] patience losses:[5.419592938125362, 5.418350346472247, 5.417304898699684, 5.416238460694551, 5.415423709595023, 5.415955349158411, 5.414490560817694, 5.414576234706442, 5.41420695636472, 5.414015456298015] min patience loss:5.414015456298015 current loss:5.412800548019878 absolute loss difference:0.0012149082781371945\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] Timing: train: 11.12s, val: 0.00s, epoch: 11.12s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869469.074303, \"EndTime\": 1616869480.1940892, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 17000000.0, \"count\": 1, \"min\": 17000000, \"max\": 17000000}, \"Total Batches Seen\": {\"sum\": 66419.0, \"count\": 1, \"min\": 66419, \"max\": 66419}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89928.50580187644 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:40 INFO 140212526044992] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:24:51.414] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 11219, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] # Finished training epoch 18 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] Loss (name: value) total: 5.412802730279107\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] Loss (name: value) kld: 0.4654939029854919\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] Loss (name: value) recons: 4.9473088277646395\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] Loss (name: value) logppx: 5.412802730279107\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=5.412802730279107\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] patience losses:[5.418350346472247, 5.417304898699684, 5.416238460694551, 5.415423709595023, 5.415955349158411, 5.414490560817694, 5.414576234706442, 5.41420695636472, 5.414015456298015, 5.412800548019878] min patience loss:5.412800548019878 current loss:5.412802730279107 absolute loss difference:2.182259228433736e-06\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] Timing: train: 11.22s, val: 0.00s, epoch: 11.22s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869480.1944048, \"EndTime\": 1616869491.4157143, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 18000000.0, \"count\": 1, \"min\": 18000000, \"max\": 18000000}, \"Total Batches Seen\": {\"sum\": 70326.0, \"count\": 1, \"min\": 70326, \"max\": 70326}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89114.95525060128 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:24:51 INFO 140212526044992] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:25:02.670] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 11253, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] # Finished training epoch 19 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] Loss (name: value) total: 5.412926895709363\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] Loss (name: value) kld: 0.46464995800731795\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] Loss (name: value) recons: 4.948276941386336\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] Loss (name: value) logppx: 5.412926895709363\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=5.412926895709363\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] patience losses:[5.417304898699684, 5.416238460694551, 5.415423709595023, 5.415955349158411, 5.414490560817694, 5.414576234706442, 5.41420695636472, 5.414015456298015, 5.412800548019878, 5.412802730279107] min patience loss:5.412800548019878 current loss:5.412926895709363 absolute loss difference:0.00012634768948505126\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] Timing: train: 11.25s, val: 0.00s, epoch: 11.26s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869491.416028, \"EndTime\": 1616869502.6713388, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 19000000.0, \"count\": 1, \"min\": 19000000, \"max\": 19000000}, \"Total Batches Seen\": {\"sum\": 74233.0, \"count\": 1, \"min\": 74233, \"max\": 74233}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=88845.88933362512 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:02 INFO 140212526044992] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:25:14.117] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 11445, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] # Finished training epoch 20 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] Loss (name: value) total: 5.411801186216924\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] Loss (name: value) kld: 0.4675083059578226\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] Loss (name: value) recons: 4.944292877103045\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] Loss (name: value) logppx: 5.411801186216924\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=5.411801186216924\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] patience losses:[5.416238460694551, 5.415423709595023, 5.415955349158411, 5.414490560817694, 5.414576234706442, 5.41420695636472, 5.414015456298015, 5.412800548019878, 5.412802730279107, 5.412926895709363] min patience loss:5.412800548019878 current loss:5.411801186216924 absolute loss difference:0.000999361802954013\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] Timing: train: 11.45s, val: 0.00s, epoch: 11.45s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869502.6716032, \"EndTime\": 1616869514.1228948, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 20000000.0, \"count\": 1, \"min\": 20000000, \"max\": 20000000}, \"Total Batches Seen\": {\"sum\": 78140.0, \"count\": 1, \"min\": 78140, \"max\": 78140}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=87325.01233891692 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:14 INFO 140212526044992] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:25:25.159] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 11036, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] # Finished training epoch 21 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] Loss (name: value) total: 5.411209576386822\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] Loss (name: value) kld: 0.4697663878193895\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] Loss (name: value) recons: 4.94144318907469\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] Loss (name: value) logppx: 5.411209576386822\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=5.411209576386822\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] patience losses:[5.415423709595023, 5.415955349158411, 5.414490560817694, 5.414576234706442, 5.41420695636472, 5.414015456298015, 5.412800548019878, 5.412802730279107, 5.412926895709363, 5.411801186216924] min patience loss:5.411801186216924 current loss:5.411209576386822 absolute loss difference:0.0005916098301019801\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] Timing: train: 11.04s, val: 0.00s, epoch: 11.04s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869514.123229, \"EndTime\": 1616869525.164881, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 21000000.0, \"count\": 1, \"min\": 21000000, \"max\": 21000000}, \"Total Batches Seen\": {\"sum\": 82047.0, \"count\": 1, \"min\": 82047, \"max\": 82047}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=90564.67994669551 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:25 INFO 140212526044992] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:25:36.314] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 11148, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] # Finished training epoch 22 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] Loss (name: value) total: 5.412607261095323\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] Loss (name: value) kld: 0.47293515403099246\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] Loss (name: value) recons: 4.9396721046233925\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] Loss (name: value) logppx: 5.412607261095323\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=5.412607261095323\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] patience losses:[5.415955349158411, 5.414490560817694, 5.414576234706442, 5.41420695636472, 5.414015456298015, 5.412800548019878, 5.412802730279107, 5.412926895709363, 5.411801186216924, 5.411209576386822] min patience loss:5.411209576386822 current loss:5.412607261095323 absolute loss difference:0.0013976847085004351\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] Timing: train: 11.15s, val: 0.00s, epoch: 11.15s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869525.1651855, \"EndTime\": 1616869536.315835, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 22000000.0, \"count\": 1, \"min\": 22000000, \"max\": 22000000}, \"Total Batches Seen\": {\"sum\": 85954.0, \"count\": 1, \"min\": 85954, \"max\": 85954}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89679.82155479386 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:36 INFO 140212526044992] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:25:47.355] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 11038, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] # Finished training epoch 23 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] Loss (name: value) total: 5.412126625533112\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] Loss (name: value) kld: 0.4712592194704012\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] Loss (name: value) recons: 4.940867404056566\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] Loss (name: value) logppx: 5.412126625533112\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=5.412126625533112\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] patience losses:[5.414490560817694, 5.414576234706442, 5.41420695636472, 5.414015456298015, 5.412800548019878, 5.412802730279107, 5.412926895709363, 5.411801186216924, 5.411209576386822, 5.412607261095323] min patience loss:5.411209576386822 current loss:5.412126625533112 absolute loss difference:0.0009170491462899832\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] Timing: train: 11.04s, val: 0.00s, epoch: 11.04s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869536.3160758, \"EndTime\": 1616869547.3564057, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 23000000.0, \"count\": 1, \"min\": 23000000, \"max\": 23000000}, \"Total Batches Seen\": {\"sum\": 89861.0, \"count\": 1, \"min\": 89861, \"max\": 89861}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=90575.76898901381 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:47 INFO 140212526044992] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:25:58.460] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 11103, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] # Finished training epoch 24 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] Loss (name: value) total: 5.411662666242935\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] Loss (name: value) kld: 0.47476501187721054\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] Loss (name: value) recons: 4.936897653614373\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] Loss (name: value) logppx: 5.411662666242935\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=5.411662666242935\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] patience losses:[5.414576234706442, 5.41420695636472, 5.414015456298015, 5.412800548019878, 5.412802730279107, 5.412926895709363, 5.411801186216924, 5.411209576386822, 5.412607261095323, 5.412126625533112] min patience loss:5.411209576386822 current loss:5.411662666242935 absolute loss difference:0.00045308985611303143\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:7\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] Timing: train: 11.10s, val: 0.00s, epoch: 11.10s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869547.3567164, \"EndTime\": 1616869558.461983, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 24000000.0, \"count\": 1, \"min\": 24000000, \"max\": 24000000}, \"Total Batches Seen\": {\"sum\": 93768.0, \"count\": 1, \"min\": 93768, \"max\": 93768}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=90046.28503019456 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:25:58 INFO 140212526044992] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:26:09.879] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 11416, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] # Finished training epoch 25 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] Loss (name: value) total: 5.411127569366057\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] Loss (name: value) kld: 0.4749122667940401\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] Loss (name: value) recons: 4.936215300678383\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] Loss (name: value) logppx: 5.411127569366057\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=5.411127569366057\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] patience losses:[5.41420695636472, 5.414015456298015, 5.412800548019878, 5.412802730279107, 5.412926895709363, 5.411801186216924, 5.411209576386822, 5.412607261095323, 5.412126625533112, 5.411662666242935] min patience loss:5.411209576386822 current loss:5.411127569366057 absolute loss difference:8.200702076521793e-05\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:8\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] Timing: train: 11.42s, val: 0.00s, epoch: 11.42s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869558.462248, \"EndTime\": 1616869569.8845081, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 25000000.0, \"count\": 1, \"min\": 25000000, \"max\": 25000000}, \"Total Batches Seen\": {\"sum\": 97675.0, \"count\": 1, \"min\": 97675, \"max\": 97675}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=87547.19696969223 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:09 INFO 140212526044992] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:26:21.001] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 11116, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] # Finished training epoch 26 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] Loss (name: value) total: 5.411532960525095\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] Loss (name: value) kld: 0.4782012159337099\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] Loss (name: value) recons: 4.933331746227576\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] Loss (name: value) logppx: 5.411532960525095\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=5.411532960525095\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] patience losses:[5.414015456298015, 5.412800548019878, 5.412802730279107, 5.412926895709363, 5.411801186216924, 5.411209576386822, 5.412607261095323, 5.412126625533112, 5.411662666242935, 5.411127569366057] min patience loss:5.411127569366057 current loss:5.411532960525095 absolute loss difference:0.00040539115903825973\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:9\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] Timing: train: 11.12s, val: 0.00s, epoch: 11.12s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869569.8848195, \"EndTime\": 1616869581.002927, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 26000000.0, \"count\": 1, \"min\": 26000000, \"max\": 26000000}, \"Total Batches Seen\": {\"sum\": 101582.0, \"count\": 1, \"min\": 101582, \"max\": 101582}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89942.09919894503 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:21 INFO 140212526044992] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2021-03-27 18:26:32.117] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 11114, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] # Finished training epoch 27 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] Loss (name: value) total: 5.412224915804447\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] Loss (name: value) kld: 0.4770659418084873\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] Loss (name: value) recons: 4.935158978040669\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] Loss (name: value) logppx: 5.412224915804447\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=5.412224915804447\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] patience losses:[5.412800548019878, 5.412802730279107, 5.412926895709363, 5.411801186216924, 5.411209576386822, 5.412607261095323, 5.412126625533112, 5.411662666242935, 5.411127569366057, 5.411532960525095] min patience loss:5.411127569366057 current loss:5.412224915804447 absolute loss difference:0.001097346438389657\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:10\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] Timing: train: 11.12s, val: 0.00s, epoch: 11.12s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869581.0032058, \"EndTime\": 1616869592.1189024, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 27000000.0, \"count\": 1, \"min\": 27000000, \"max\": 27000000}, \"Total Batches Seen\": {\"sum\": 105489.0, \"count\": 1, \"min\": 105489, \"max\": 105489}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89961.76473143137 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] \u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:32 INFO 140212526044992] # Starting training for epoch 28\u001b[0m\n",
      "\n",
      "2021-03-27 18:26:48 Uploading - Uploading generated training model\u001b[34m[2021-03-27 18:26:43.335] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 11216, \"num_examples\": 3907, \"num_bytes\": 43926248}\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] # Finished training epoch 28 on 1000000 examples from 3907 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Loss (name: value) total: 5.411982710903979\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Loss (name: value) kld: 0.47623247444431344\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Loss (name: value) recons: 4.935750238511579\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Loss (name: value) logppx: 5.411982710903979\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=5.411982710903979\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] patience losses:[5.412802730279107, 5.412926895709363, 5.411801186216924, 5.411209576386822, 5.412607261095323, 5.412126625533112, 5.411662666242935, 5.411127569366057, 5.411532960525095, 5.412224915804447] min patience loss:5.411127569366057 current loss:5.411982710903979 absolute loss difference:0.0008551415379223215\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Bad epoch: loss has not improved (enough). Bad count:11\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Timing: train: 11.22s, val: 0.00s, epoch: 11.22s\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869592.1191676, \"EndTime\": 1616869603.337365, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 28000000.0, \"count\": 1, \"min\": 28000000, \"max\": 28000000}, \"Total Batches Seen\": {\"sum\": 109396.0, \"count\": 1, \"min\": 109396, \"max\": 109396}, \"Max Records Seen Between Resets\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Max Batches Seen Between Resets\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 1000000.0, \"count\": 1, \"min\": 1000000, \"max\": 1000000}, \"Number of Batches Since Last Reset\": {\"sum\": 3907.0, \"count\": 1, \"min\": 3907, \"max\": 3907}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] #throughput_metric: host=algo-1, train throughput=89139.67659476941 records/second\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 WARNING 140212526044992] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Best model based on early stopping at epoch 25. Best loss: 5.411127569366057\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Topics from epoch:final (num_topics:10) [wetc 0.40, tu 0.82]:\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] [0.49, 0.84] stabbing charged murder fatal man guilty pleads bail jailed assault alleged arrested teen shooting crash accident raid front girl found\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] [0.25, 0.70] hears battle spark nrl prompt crew afl john medium blaze firefighter air rule smith interview house resident front legal inquest\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] [0.26, 0.78] abc sport news pleads guilty rural weather speaks national business closer john pacific found park nrl nrn interview market beat\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] [0.33, 0.71] teacher firefighter blaze labor candidate liberal denies reject pay coast gold sex hour former poll battle interest rate country vote\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] [0.40, 0.91] seeker asylum search missing body found boat coal island gold continues coast rescue clean sea gas miner tourist christmas near\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] [0.36, 0.95] korea troop iraq soldier iraqi kill nuclear blast bomb closer pakistan pm un u killed aust war visit trade howard\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] [0.52, 0.89] cup v world league match one miss final tiger title win victory england nrl aussie afl player second star day\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] [0.44, 0.73] hour north country firefighter wind rain crew korea damage crash plane central cyclone coast blaze fatal truck south cause east\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] [0.34, 0.76] hour country climate live investigate medium carbon one prompt v change arrest export tax hunt liberal highlight ta call policy\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] [0.58, 0.88] toll share dollar rise market profit rate interest price fall drop export carbon oil despite cattle loss bank record strong\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Saved checkpoint to \"/tmp/tmp9qbxdfe_/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[03/27/2021 18:26:43 INFO 140212526044992] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1616869270.1050246, \"EndTime\": 1616869603.3896883, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 18133.548974990845, \"count\": 1, \"min\": 18133.548974990845, \"max\": 18133.548974990845}, \"epochs\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"early_stop.time\": {\"sum\": 69.80514526367188, \"count\": 28, \"min\": 0.19669532775878906, \"max\": 4.510641098022461}, \"update.time\": {\"sum\": 315078.23967933655, \"count\": 28, \"min\": 11040.148496627808, \"max\": 11561.170816421509}, \"finalize.time\": {\"sum\": 46.51165008544922, \"count\": 1, \"min\": 46.51165008544922, \"max\": 46.51165008544922}, \"model.serialize.time\": {\"sum\": 4.459619522094727, \"count\": 1, \"min\": 4.459619522094727, \"max\": 4.459619522094727}, \"setuptime\": {\"sum\": 111.67669296264648, \"count\": 1, \"min\": 111.67669296264648, \"max\": 111.67669296264648}, \"totaltime\": {\"sum\": 333438.98487091064, \"count\": 1, \"min\": 333438.98487091064, \"max\": 333438.98487091064}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-03-27 18:26:55 Completed - Training job completed\n",
      "Training seconds: 407\n",
      "Billable seconds: 407\n"
     ]
    }
   ],
   "source": [
    "ntm.fit(inputs={'train': s3_training_path,\n",
    "                'auxiliary': s3_auxiliary_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['justice','finance','local','sports','politics?',\n",
    "          'unknown1','unknown2','crime','disasters', 'international']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.t2.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_samples(samples, dictionary):\n",
    "    num_lines = len(samples)\n",
    "    num_columns = len(dictionary)\n",
    "    sample_matrix = np.zeros((num_lines, num_columns)).astype('float32')\n",
    "    for line in range(0, num_lines):\n",
    "        s = samples[line]\n",
    "        s = process_text(s)\n",
    "        s = dictionary.doc2bow(s)\n",
    "        for token_id, token_count in s:\n",
    "            sample_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "    return sample_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to try your own samples\n",
    "\n",
    "samples = [\n",
    "    \"Major tariffs expected to end Australian barley trade to China\",\n",
    "    \"US woman wanted over fatal crash asks for release after coronavirus halts extradition\",\n",
    "    \"Fifty trains out of service as fault forces Adelaide passengers to pack like sardines\",\n",
    "    \"Germany's Bundesliga plans its return from lockdown as football world watches\",\n",
    "    \"RFS volunteer in custody for allegedly lighting fires\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load random samples from the dataset\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv.gz', compression='gzip',\n",
    "                      error_bad_lines=False, dtype='str', nrows=num_lines)\n",
    "samples = data.sample(frac=0.001)\n",
    "samples = np.array(samples.headline_text)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"predictions\":[{\"topic_weights\":[0.0366471261,0.0508532301,0.0554300584,0.0420520604,0.0667201504,0.2473417073,0.1098412201,0.0530197471,0.0677292496,0.2703654766]},{\"topic_weights\":[0.3334089518,0.0613059364,0.0527985059,0.0520550422,0.0555767864,0.0837655291,0.0492441952,0.1960011274,0.0574527942,0.0583911054]},{\"topic_weights\":[0.1040436849,0.1128442958,0.0867721066,0.0695143864,0.0790253133,0.081341207,0.0940319598,0.2365107089,0.0710377619,0.0648785681]},{\"topic_weights\":[0.058230754,0.0997985378,0.0755585134,0.0814882964,0.124075897,0.0769889504,0.2596221566,0.0771221071,0.0699055642,0.0772091597]},{\"topic_weights\":[0.094705008,0.1300486177,0.0844352543,0.0948471427,0.0922646448,0.0781801119,0.0744621903,0.1861951351,0.087393932,0.0774679929]}]}'\n"
     ]
    }
   ],
   "source": [
    "ntm_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "response = ntm_predictor.predict(process_samples(samples, dictionary))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('international', 0.2703654766), ('unknown1', 0.2473417073), ('unknown2', 0.1098412201)]\n",
      "[('justice', 0.3334089518), ('crime', 0.1960011274), ('unknown1', 0.0837655291)]\n",
      "[('crime', 0.2365107089), ('finance', 0.1128442958), ('justice', 0.1040436849)]\n",
      "[('unknown2', 0.2596221566), ('politics?', 0.124075897), ('finance', 0.0997985378)]\n",
      "[('crime', 0.1861951351), ('finance', 0.1300486177), ('sports', 0.0948471427)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = json.loads(response)\n",
    "\n",
    "for r in response['predictions']:\n",
    "    sorted_indexes = np.argsort(r['topic_weights']).tolist()\n",
    "    sorted_indexes.reverse()\n",
    "    top_topics = [topics[i] for i in sorted_indexes]\n",
    "    top_weights = [r['topic_weights'][i] for i in sorted_indexes]\n",
    "    pairs = list(zip(top_topics, top_weights))\n",
    "    print(pairs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'topic_weights': [0.0366471261,\n",
      "                                    0.0508532301,\n",
      "                                    0.0554300584,\n",
      "                                    0.0420520604,\n",
      "                                    0.0667201504,\n",
      "                                    0.2473417073,\n",
      "                                    0.1098412201,\n",
      "                                    0.0530197471,\n",
      "                                    0.0677292496,\n",
      "                                    0.2703654766]},\n",
      "                 {'topic_weights': [0.3334089518,\n",
      "                                    0.0613059364,\n",
      "                                    0.0527985059,\n",
      "                                    0.0520550422,\n",
      "                                    0.0555767864,\n",
      "                                    0.0837655291,\n",
      "                                    0.0492441952,\n",
      "                                    0.1960011274,\n",
      "                                    0.0574527942,\n",
      "                                    0.0583911054]},\n",
      "                 {'topic_weights': [0.1040436849,\n",
      "                                    0.1128442958,\n",
      "                                    0.0867721066,\n",
      "                                    0.0695143864,\n",
      "                                    0.0790253133,\n",
      "                                    0.081341207,\n",
      "                                    0.0940319598,\n",
      "                                    0.2365107089,\n",
      "                                    0.0710377619,\n",
      "                                    0.0648785681]},\n",
      "                 {'topic_weights': [0.058230754,\n",
      "                                    0.0997985378,\n",
      "                                    0.0755585134,\n",
      "                                    0.0814882964,\n",
      "                                    0.124075897,\n",
      "                                    0.0769889504,\n",
      "                                    0.2596221566,\n",
      "                                    0.0771221071,\n",
      "                                    0.0699055642,\n",
      "                                    0.0772091597]},\n",
      "                 {'topic_weights': [0.094705008,\n",
      "                                    0.1300486177,\n",
      "                                    0.0844352543,\n",
      "                                    0.0948471427,\n",
      "                                    0.0922646448,\n",
      "                                    0.0781801119,\n",
      "                                    0.0744621903,\n",
      "                                    0.1861951351,\n",
      "                                    0.087393932,\n",
      "                                    0.0774679929]}]}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (SageMaker JumpStart Data Science 1.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:793310587911:image/sagemaker-jumpstart-data-science-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
